\documentclass{article}

\usepackage{amsmath}
\usepackage{geometry}
\usepackage{natbib}

\title{Proportions of Variants of Concern in NML Wastewater Data}
\author{Devan Becker, others}
\date{}




\begin{document}

\maketitle

\begin{abstract}
Wastewater is good for many things.
Others have done some variant detection via various methods.
Our method is principled, statistically sound, and cool.
Mutation lists are an NP-hard problem (not actually, but basically).
The method performs great, package is set up in a convenient way (TODO: set up package in a convenient way).
\end{abstract}

\section{Introduction}

Did you know there was a pandemic?
Wastewater has been shown to be useful for many analyses and pandemic response policies.

There are a few approaches to detection of SARS-CoV-2 variants\footnote{We use the convention that ``variant'' refers to a particular subtype of SARS-CoV-2 that is of interest to health authorities, such as the WHO's designated Variants of Concern (VoC) or Variants of Interest (VoI), and ``lineage'' refers to any subtype of SARS-CoV-2, whether it's designated a VOC, VOI, or neither.} in wastewater.
PCR-based search for particular variants when there are very specific mutations that are uniquely defining and easy to target.
For instance, the S-Gene Target Failure (SGTF) uniquely defines the BA.1 subvariant of B.1.1.529 (Omicron), but is not present in BA.2.
Detection of SGTF can be done without performing a full sequencing of a wastewater sample, and was useful in the detection of Omicron.

The remaining methods rely on full sequencing of the wastewater sample.
Research is still being performed on optimal sequencing strategies.
For a thorough discussion, do your own literature review. TODO: References for wastewater sequencing.
Under the ARTIC protocol (one of the most commonly used sequencing strategies), sequencing is performed using 48 amplicon regions of approximately 400bp each.
This results in a short read file of many reads that may or may not come from the same genome\footnote{It is essentially impossible to obtain and sequence any complete genomes from wastewater}.
Most common processing pipelines result in a list of mutations (relative to some reference genome, often Wuhan-1), along with the number of times those mutations were observed.
The read depth at each position relative to a reference is also obtained.
For the rest of this paper, we are primarily concerned with these data, which we will refer to as counts and coverage, respectively.

Instead of a targeted sequencing analysis (such as searching for SGTF), one can search the list of mutations for lineage-defining mutations.
Given a list of mutations that are believed to be unique (or sufficiently representative) to one lineage, the average of the observed frequencies\footnote{Frequency here is defined as the count divided by the coverage at that location.} of the mutations can be reported as an estimate of the abundance of that lineage \citep{chrystal's preprint}.
The counts and coverage can be seen as ``successes'' and ``sample size'' in a binomial model, which allows for something like a weighted average of frequencies along with a confidence interval.
This model can be found in GromStole, where the authors incorporate overdispersion for a more robust estimate of the confidence interval limits.

We are aware of two methods for ``deconvolution'' or ``de-mixing'' of mutations and variants.
Both methods are based on the idea that the frequency of a mutation should be approximately equal to the sum of the proportions of lineages present in the wastewater that contain that mutation.
For instance, if mutation A has a frequency of 0.75 and both Lineage 1 and Lineage 2 are known to have mutation A, then the porportion of Lineage 1 plus the proportion of Lineage 2 should (ideally) sum to 0.75 (but in practice there will be some error).
Given a sufficiently large list of mutations and assuming an appropriate error structure, it is possible to determine the relative frequencies of different lineages.

Freyja \citep{freyja}{part of iVar} and Alcov implement a linear regression based approach to deconvolution of mutation frequencies.
Alcov uses an unconstrained linear regression approach, which is both easy to interpret and provides reasonable estimates despite not constraining the proportions to be positive or to sum to 1.
Freya uses a robust approach and includes constraints to ensure positivity and sum-to-one.
Both methods are parts of sophisticated sequence processing pipelines.

In this paper, we introduce and implement an approach to deconvolution based on the binomial model to properly accommodate the error structure and implements positivity and sum-to-one constraints for valid estimates of proportions of lineages.
The method is implemented in both a Bayesian and a constrained optimization framework, both of which have advantages for different situations.
This method is a statistically sound and principled approach to estimation of proportions of lineages and is implemented in a user-friendly R package.

\section{Background}

\subsection{The Variant Matrix}

We define the variant matrix $M$ as a matrix where the columns represent mutations and the rows represent variants.
The $i,j$ entry is a 1 if mutation $j$ is present in variant $i$ and 0 otherwise.
This provides a very convenient data structure for analysis which allows for matrix multiplication.
We emphasize the importance of the variant matrix.
The variants that are included and the mutations associated with those variants are of paramount importance, and we stress that poor specification can have a large impact on the results of any deconvolution algorithm.

% TODO: Equivalent variants given data

\subsection{Alcove and Freyja}

In this section, we describe the methods of Alcove and Freyja in general terms.
While not emphasizing its use, both approaches use a form of what we call the variant matrix. 
Both methods rely on the functional form:

\begin{equation}
\text{\underline{Frequency}} = \underline \rho M + \underline \epsilon \label{OLS}
\end{equation}

\noindent where \underline{Frequency} is the vector of frequencies, $\underline \rho$ is the vector of proportions of each lineage, $M$ is the variant matrix, and $\underline \epsilon$ is the vector of errors.
Alcov uses a closed form solution to ordinary least squares optimization of $\underline \rho$ in Equation \ref{OLS}.
Freyja imposes positivity and sum-to-one constraints on $\rho$ and implements a bootstrap algorithm to robustly estimate a confidence interval around the estimates.

\section{ProVoC}

We introduce ProVoC, which implements a binomial error structure that properly incorporates the information from coverage and imposes the positivity and sum-to-one constraints.
We first demonstrate the features of this method using a motivating example, then provide the specificiation of the model.

\subsection{Example}

To motivate our model choices, consider the following example.
Table \ref{toy_data} represents the format of our input data, showing the count and coverage of each mutation observed in the data.
The data also contain mutation aa:orf1a:S135R, which was \emph{not} observed in our data.
However, it is a mutation in one of the variants of concern and we were able to obtain the coverage at its position using the read depth for all positions (e.g., the command \texttt{samtools depth -aa}).

\begin{table}
\begin{tabular}{l|l|l}
Mutation & Count & Coverage\\\hline
aa:E:T91 & 1 & 2\\
aa:M:Q19E & 73 & 100\\
25000T & 387 & 1034\\
aa:orf1a:S135R & 0 & 3385
\end{tabular}
\caption{\label{toy_data}An example data set to motivate the model. Each row represents a single mutation, with the counts representing the number of times that mutation was observed and the coverage representing the read depth at that position.}
\end{table}

The variant matrix that we have chosen is shown in Table \ref{toy_varmat}. 
For this example we are only considering Omicron (B.1.1.529) and its subvariants BA.1 and BA.2.
The existence/absence of these mutations are based on the ``constellation'' lists provided by the PANGO team (\url{https://github.com/cov-lineages/constellations}).
This means that the mutation lists mostly consist of representative mutations; if a mutation is not on the list, it still may have been observed within that lineage but been removed because it was also present in other variants of concern.
As can be seen in Tab;e \ref{toy_varmat}, there are still overlaps in the mutation lists.

\begin{table}
\begin{tabular}{l|rrrrr}
& aa:E:T9I & aa:M:Q19E & 25000T & aa:orf1a:S135R\\\hline
B.1.1.529 & 1 & 1 & 0 & 0\\
BA.1 & 0 & 0 & 1 & 0\\
BA.2 & 0 & 0 & 1 & 1
\end{tabular}
\caption{\label{toy_varmat}An example of a variant matrix. The mutation list for the B.1.1.529 variant contains mutations aa:E:T9I and aa:M:Q19, hence the 1's in these columns, but does not contain the mutations 25000T or aa:orf1a:S135R.}
\end{table}

Consider the estimation of the proportion of B.1.1.529.
It contains the mutation aa:E:T9I, which was observed in 50\% of reads.
It also contains the mutation aa:M:Q19E, which was present in 73\% of reads.
Given this information alone, we may suppose that the proportion is halfway between these two proportions.
However, the 50\% is only based on 2 reads, while the 73\% is based on 100 reads.
An estimate of the proportion should be weighted higher when the read depth is higher.

Now lets consider BA.1 and BA.2.
These both contain the mutation 25000T, which is present in 37\% of the reads.
From this, we can guess that the proportion of BA.1 plus the proportion of BA.2 adds to approximately 37\%.
We also know that the mutation aa:orf1a:S135R was not observed in any reads and had a coverage of 3385, meaning that this mutation is very likely not present.
Since BA.2 contains this mutation, it is unlikely that BA.2 is present.
Since the proportions of BA.1 and BA.2 sum to 37\% and BA.2 is likely not present, BA.1 must have a proportion near 37\%.

This example highlights several features that our model includes.
First, we incorporate coverage information to give higher weight to mutations with higher coverage.
Where other methods remove mutations with low coverage, we simply give them very low weight in the estimation.
Second, we allow for shared mutations within mutation lists.
Third, we use information from mutations that were not present in the data.
A mutation that was not observed but has high coverage is evidence against the presence of a lineage, and this information can be leveraged to improve the estimate.




\subsection{Model Specification}

Our model assumes a binomial distribution for the counts (not the frequencies), where the binomial probability is calculated according to $\underline \rho X$ and the sample size is the coverage at the relevant position on the reference:

$$
\begin{aligned}
\text{Count}_i &\sim \text{Binom}(p_i, \text{Coverage}_i)\\
\underline p &= \underline M \times \rho
\end{aligned}
$$

In this formulation, $\rho$ is a latent vector that must be estimated.
Estimation is performed in two different ways.
A constrained optimization algorithm is implemented, which fits very quickly and a bootstrap resampling scheme allows for estimation of confidence intervals.
The constraints on this algorithm ensure that the proportions are positive and the sum of the proportions is between 0 and 1.
The 

A Bayesian model allows for inspection of posterior distributions and posterior covariance, as well as providing a basis for more complex analyses (see Conclusions and Future Work).
Note that the Bayesian model specification includes a prior sampling scheme to impose the positivity and sum-to-one constraints on $\rho$.
In particular, if a proportion is sampled that causes the sum to be greater than one, it is rescaled so that the sum is exactly one.
This sampling ensures that the sum does not exceed 1, but is allowed to be less than 1. 
The prior is a beta distribution with parameters $\alpha=2$ and $\beta=8$, ensuring positivity of the proportions and low starting proportions for all lineages.





\subsection{Residual Analysis}

After obtaining an estimate of $\underline\rho$, which we label $\hat{\underline\rho}$, it is possible to obtain the expected counts of each mutation using the equation:
$$\widehat{\text{\underline{Count}}} = \text{\underline{Coverage}}\odot\left(M\times\hat{\underline\rho}\right)$$

\noindent where $\odot$ represents Hadamard (element-wise) multiplication.

These residuals can hold important clues for potential misspecification of the variant matrix.
If there are mutations that have high prevalence in the data but low prevalence in the expected counts, this might indicate that it is a representative mutation from a lineage that is not present in the data. 

\section{Choosing Mutation Lists}

\section{Simulation Study}

To ensure our method is performing as expected, we have set up a simulation study that mimics real-world data.
In this study, we compare our method to a re-implementation of Freyja and Alcov in the R programming language.
We also compare our method against versions of Freyja and Alcov that use counts rather than frequencies (and thus use the binomial distribution), and apply a bootstrapping approach to obtain confidence intervals.
Finally, we include a heuristic approach that estimates the proportion of a lineage using the average of representative mutations.

\subsection{Simulating Wastewater Samples}

To mimic real-world data, we use the following simulation setup.
Instead of sampling from our mutation lists (which would ignore uncommon mutations and sequencing error), we sample from known sequences in the GenBank database.
We assume that there is a true number of complete sequences in the wastewater, but this number is not something that we attempt to estimate.
Instead, we sample a known proportion of the GenBank sequences that were assigned to a given lineage.
For example, we may take 400 sequences at random from those labelled B.1.1.529 and 600 sequences labelled BA.1.

From this collection of sequences, we can calculate the frequency of each mutation.
We multiply these frequencies by a randomly chosen coverage map from our data in order to simulate a realistic coverage map.
The result of this multiplication is a simulated set of counts of a simulated set of mutations, but with a fixed coverage map.

We believe that this setup is realistic for several reasons.
The mutations are sampled from real sequences which circumvents the need to have a mutation list.
Because of this, mutations that only happen in a few sequences will still make it into our data.
The use of real-life coverage maps means that we properly emulate the coverage obtained via the ARTIC protocol (which is the industry standard despite having potential lack of coverage of the spike protein) after RNA degredation (which recent studies have found to not be uniform across the genome) 

\subsection{Parameter Setup}

To simulate the data, we only need to choose the number of sequences of each lineage that we are sampling.
We consider a gradient between Delta (and its sublineages) and Omicron (and its sublineages), as shown in the Figure \ref{deltacron}.
These prorportions are based on \cite{David's invasion paper}.
To emulate sequencing error, 1\% of each sample is from the rest of the GenBank data (i.e., neither Delta nor Omicron).
To emulate the conditions of the start of a transition, we will restrict our sequences to those collected prior to the dates shown in the plot.

\begin{figure}
Delta to Omicron
\caption{\label{deltacron}The proportion of each lineage in the simulated data. 1\% of all sequences sampled will be ``Other'' to emulate sequencing error.}
\end{figure}

Constructing the mutation lists is non-trivial.
Thanks to the version control system (git/GitHub) used by the PANGO team, we are able to ``go back in time'' and use the mutation lists available at the given dates.
For the mutation lists based on the GenBank data, we can subset the GenBank data to what was available at the time.
However, GenBank's labels have changed over time and we cannot emulate this.


\subsection{Results}

The results are fourthcoming... thirdcoming... secondcoming... firstcoming... INCOMING!




\section{Application}

\subsection{Data Description}

\subsection{Results}

(By variantmat type - known lineages versus constellations)

\subsubsection{Sensitivity to Inclusion of Lineages}

\section{Conclusions and Future Work}


We are actively pursuing a version of the Bayesian model which imposes similarity of proportions based on distance in time and in space.

\end{document}
